{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike rental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.appName('BikeRentals').getOrCreate()\n",
    "\n",
    "# load bike rental dataset\n",
    "df = spark.read.csv(\"bike_rental_dataset.csv\", inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|dayOfWeek|days|demand|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|      Sat|   0|    16|\n",
      "|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    40|\n",
      "|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    32|\n",
      "|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|    13|\n",
      "|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     1|\n",
      "|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|      Sat|   0|     1|\n",
      "|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|     2|\n",
      "|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|      Sat|   0|     3|\n",
      "|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     8|\n",
      "|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|      Sat|   0|    14|\n",
      "|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|      Sat|   0|    36|\n",
      "|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|      Sat|   0|    56|\n",
      "|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|      Sat|   0|    84|\n",
      "|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|      Sat|   0|    94|\n",
      "|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|      Sat|   0|   106|\n",
      "|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|      Sat|   0|   110|\n",
      "|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|      Sat|   0|    93|\n",
      "|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|      Sat|   0|    67|\n",
      "|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    35|\n",
      "|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    37|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show dataframes when the day is monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+----------+----------+----+----+-------------------+---------+----+------+\n",
      "|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|          windspeed|dayOfWeek|days|demand|\n",
      "+------+---+----+---+-------+----------+----------+----+----+-------------------+---------+----+------+\n",
      "|     1|  0|   1|  0|      0|         1|         1|0.22|0.44|             0.3582|      Mon|   1|     5|\n",
      "|     1|  0|   1|  1|      0|         1|         1| 0.2|0.44|             0.4179|      Mon|   2|     2|\n",
      "|     1|  0|   1|  4|      0|         1|         1|0.16|0.47|             0.3881|      Mon|   2|     1|\n",
      "|     1|  0|   1|  5|      0|         1|         1|0.16|0.47|             0.2836|      Mon|   2|     3|\n",
      "|     1|  0|   1|  6|      0|         1|         1|0.14| 0.5|             0.3881|      Mon|   2|    30|\n",
      "|     1|  0|   1|  7|      0|         1|         1|0.14| 0.5|0.19399999999999998|      Mon|   2|    64|\n",
      "|     1|  0|   1|  8|      0|         1|         1|0.14| 0.5|             0.2836|      Mon|   2|   154|\n",
      "|     1|  0|   1|  9|      0|         1|         1|0.16|0.43|             0.3881|      Mon|   2|    88|\n",
      "|     1|  0|   1| 10|      0|         1|         1|0.18|0.43|             0.2537|      Mon|   2|    44|\n",
      "|     1|  0|   1| 11|      0|         1|         1| 0.2| 0.4|             0.3284|      Mon|   2|    51|\n",
      "|     1|  0|   1| 12|      0|         1|         1|0.22|0.35|             0.2985|      Mon|   2|    61|\n",
      "|     1|  0|   1| 13|      0|         1|         1|0.24|0.35|             0.2836|      Mon|   2|    61|\n",
      "|     1|  0|   1| 14|      0|         1|         1|0.26| 0.3|             0.2836|      Mon|   2|    77|\n",
      "|     1|  0|   1| 15|      0|         1|         1|0.26| 0.3|             0.2537|      Mon|   2|    72|\n",
      "|     1|  0|   1| 16|      0|         1|         1|0.26| 0.3|             0.2537|      Mon|   2|    76|\n",
      "|     1|  0|   1| 17|      0|         1|         1|0.24| 0.3|             0.2239|      Mon|   2|   157|\n",
      "|     1|  0|   1| 18|      0|         1|         1|0.24|0.32|             0.1045|      Mon|   2|   157|\n",
      "|     1|  0|   1| 19|      0|         1|         1| 0.2|0.47|                0.0|      Mon|   2|   110|\n",
      "|     1|  0|   1| 20|      0|         1|         1| 0.2|0.47|             0.1045|      Mon|   2|    52|\n",
      "|     1|  0|   1| 21|      0|         1|         1|0.18|0.64|             0.1343|      Mon|   2|    52|\n",
      "+------+---+----+---+-------+----------+----------+----+----+-------------------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.dayOfWeek==\"Mon\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- dayOfWeek: string (nullable = true)\n",
      " |-- days: integer (nullable = true)\n",
      " |-- demand: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show number reservation by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|dayOfWeek|count|\n",
      "+---------+-----+\n",
      "|      Sun| 2502|\n",
      "|      Mon| 2479|\n",
      "|      Sat| 2512|\n",
      "|      Wed| 2475|\n",
      "|      Tue| 2453|\n",
      "|      Fri| 2487|\n",
      "|      Thr| 2471|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dayOfWeek').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show number reservation by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|mnth|count|\n",
      "+----+-----+\n",
      "|  12| 1483|\n",
      "|   1| 1429|\n",
      "|   6| 1440|\n",
      "|   3| 1473|\n",
      "|   5| 1488|\n",
      "|   9| 1437|\n",
      "|   4| 1437|\n",
      "|   8| 1475|\n",
      "|   7| 1488|\n",
      "|  10| 1451|\n",
      "|  11| 1437|\n",
      "|   2| 1341|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('mnth').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show number reservation by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|mnth|count|\n",
      "+----+-----+\n",
      "|  12| 1483|\n",
      "|   1| 1429|\n",
      "|   6| 1440|\n",
      "|   3| 1473|\n",
      "|   5| 1488|\n",
      "|   9| 1437|\n",
      "|   4| 1437|\n",
      "|   8| 1475|\n",
      "|   7| 1488|\n",
      "|  10| 1451|\n",
      "|  11| 1437|\n",
      "|   2| 1341|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('mnth').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show number reservation by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| yr|count|\n",
      "+---+-----+\n",
      "|  1| 8734|\n",
      "|  0| 8645|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('yr').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show number reservation by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|season|count|\n",
      "+------+-----+\n",
      "|     1| 4242|\n",
      "|     3| 4496|\n",
      "|     4| 4232|\n",
      "|     2| 4409|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('season').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='dayOfWeek', outputCol='day_cat')\n",
    "indexed_data = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+-------+\n",
      "|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|dayOfWeek|days|demand|day_cat|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+-------+\n",
      "|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|      Sat|   0|    16|    0.0|\n",
      "|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    40|    0.0|\n",
      "|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    32|    0.0|\n",
      "|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|    13|    0.0|\n",
      "|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     1|    0.0|\n",
      "|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|      Sat|   0|     1|    0.0|\n",
      "|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|     2|    0.0|\n",
      "|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|      Sat|   0|     3|    0.0|\n",
      "|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     8|    0.0|\n",
      "|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|      Sat|   0|    14|    0.0|\n",
      "|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|      Sat|   0|    36|    0.0|\n",
      "|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|      Sat|   0|    56|    0.0|\n",
      "|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|      Sat|   0|    84|    0.0|\n",
      "|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|      Sat|   0|    94|    0.0|\n",
      "|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|      Sat|   0|   106|    0.0|\n",
      "|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|      Sat|   0|   110|    0.0|\n",
      "|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|      Sat|   0|    93|    0.0|\n",
      "|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|      Sat|   0|    67|    0.0|\n",
      "|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    35|    0.0|\n",
      "|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    37|    0.0|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|day_cat|count|\n",
      "+-------+-----+\n",
      "|    0.0| 2512|\n",
      "|    1.0| 2502|\n",
      "|    2.0| 2487|\n",
      "|    3.0| 2479|\n",
      "|    4.0| 2475|\n",
      "|    5.0| 2471|\n",
      "|    6.0| 2453|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_data.groupBy('day_cat').count().orderBy('day_cat').show(n=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the first model that is given to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'yr',\n",
       " 'mnth',\n",
       " 'hr',\n",
       " 'holiday',\n",
       " 'workingday',\n",
       " 'weathersit',\n",
       " 'temp',\n",
       " 'hum',\n",
       " 'windspeed',\n",
       " 'dayOfWeek',\n",
       " 'days',\n",
       " 'demand',\n",
       " 'day_cat']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: int, yr: int, mnth: int, hr: int, holiday: int, workingday: int, weathersit: int, temp: double, hum: double, windspeed: double, dayOfWeek: string, days: int, demand: int, day_cat: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = VectorAssembler(\n",
    "  inputCols= [\n",
    "    'season',\n",
    "    'yr',\n",
    "    'mnth',\n",
    "    'hr',\n",
    "    'holiday',\n",
    "    'workingday',\n",
    "    'weathersit',\n",
    "    'temp',\n",
    "    'hum',\n",
    "    'windspeed',\n",
    "    'day_cat'\n",
    "    ],\n",
    "   outputCol = 'features'                  \n",
    ")\n",
    "data = vec.transform(indexed_data)\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0.24\n",
      "0.81\n",
      "0.0\n",
      "Sat\n",
      "0\n",
      "16\n",
      "0.0\n",
      "(11,[0,2,6,7,8],[1.0,1.0,1.0,0.24,0.81])\n"
     ]
    }
   ],
   "source": [
    "for item in data.take(1)[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(season=1, yr=0, mnth=1, hr=0, holiday=0, workingday=0, weathersit=1, temp=0.24, hum=0.81, windspeed=0.0, dayOfWeek='Sat', days=0, demand=16, day_cat=0.0, features=SparseVector(11, {0: 1.0, 2: 1.0, 6: 1.0, 7: 0.24, 8: 0.81}))\n",
      "\n",
      "Row(season=1, yr=0, mnth=1, hr=1, holiday=0, workingday=0, weathersit=1, temp=0.22, hum=0.8, windspeed=0.0, dayOfWeek='Sat', days=0, demand=40, day_cat=0.0, features=SparseVector(11, {0: 1.0, 2: 1.0, 3: 1.0, 6: 1.0, 7: 0.22, 8: 0.8}))\n",
      "\n",
      "Row(season=1, yr=0, mnth=1, hr=2, holiday=0, workingday=0, weathersit=1, temp=0.22, hum=0.8, windspeed=0.0, dayOfWeek='Sat', days=0, demand=32, day_cat=0.0, features=SparseVector(11, {0: 1.0, 2: 1.0, 3: 2.0, 6: 1.0, 7: 0.22, 8: 0.8}))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in data.take(3):\n",
    "    print(item, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get features column and demand column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------+\n",
      "|features                                           |demand|\n",
      "+---------------------------------------------------+------+\n",
      "|(11,[0,2,6,7,8],[1.0,1.0,1.0,0.24,0.81])           |16    |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,1.0,1.0,0.22,0.8])      |40    |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,2.0,1.0,0.22,0.8])      |32    |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,3.0,1.0,0.24,0.75])     |13    |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,4.0,1.0,0.24,0.75])     |1     |\n",
      "|[1.0,0.0,1.0,5.0,0.0,0.0,2.0,0.24,0.75,0.0896,0.0] |1     |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,6.0,1.0,0.22,0.8])      |2     |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,7.0,1.0,0.2,0.86])      |3     |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,8.0,1.0,0.24,0.75])     |8     |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,9.0,1.0,0.32,0.76])     |14    |\n",
      "|[1.0,0.0,1.0,10.0,0.0,0.0,1.0,0.38,0.76,0.2537,0.0]|36    |\n",
      "|[1.0,0.0,1.0,11.0,0.0,0.0,1.0,0.36,0.81,0.2836,0.0]|56    |\n",
      "|[1.0,0.0,1.0,12.0,0.0,0.0,1.0,0.42,0.77,0.2836,0.0]|84    |\n",
      "|[1.0,0.0,1.0,13.0,0.0,0.0,2.0,0.46,0.72,0.2985,0.0]|94    |\n",
      "|[1.0,0.0,1.0,14.0,0.0,0.0,2.0,0.46,0.72,0.2836,0.0]|106   |\n",
      "|[1.0,0.0,1.0,15.0,0.0,0.0,2.0,0.44,0.77,0.2985,0.0]|110   |\n",
      "|[1.0,0.0,1.0,16.0,0.0,0.0,2.0,0.42,0.82,0.2985,0.0]|93    |\n",
      "|[1.0,0.0,1.0,17.0,0.0,0.0,2.0,0.44,0.82,0.2836,0.0]|67    |\n",
      "|[1.0,0.0,1.0,18.0,0.0,0.0,3.0,0.42,0.88,0.2537,0.0]|35    |\n",
      "|[1.0,0.0,1.0,19.0,0.0,0.0,3.0,0.42,0.88,0.2537,0.0]|37    |\n",
      "+---------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data = data.select('features', 'demand')\n",
    "model_data.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split randomly datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|             17379|\n",
      "|   mean|189.46308763450142|\n",
      "| stddev| 181.3875990918646|\n",
      "|    min|                 1|\n",
      "|    max|               977|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|             12035|\n",
      "|   mean|189.87486497714997|\n",
      "| stddev|181.07587853298674|\n",
      "|    min|                 1|\n",
      "|    max|               976|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|              5344|\n",
      "|   mean|188.53574101796409|\n",
      "| stddev| 182.1012284855826|\n",
      "|    min|                 1|\n",
      "|    max|               977|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, demand: int]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 12:33:01 WARN Instrumentation: [4553b589] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance = 12549.328489263007\n",
      "mean absolute error = 106.26800667712139\n"
     ]
    }
   ],
   "source": [
    "summary = lr_model.summary\n",
    "print(\"explained variance =\", summary.explainedVariance)\n",
    "print(\"mean absolute error =\", summary.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38276777972947296"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------+-------------------+\n",
      "|features                                       |demand|prediction         |\n",
      "+-----------------------------------------------+------+-------------------+\n",
      "|(11,[0,1,2,6,7,8],[2.0,1.0,5.0,1.0,0.6,0.83])  |153.0 |116.90169778316846 |\n",
      "|(11,[0,1,2,6,7,8],[3.0,1.0,8.0,2.0,0.7,0.61])  |135.0 |202.53209304747514 |\n",
      "|(11,[0,1,2,6,7,8],[4.0,1.0,12.0,1.0,0.26,0.81])|108.0 |62.91089805521192  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,1.0,1.0,0.22,0.8])  |40.0  |-75.93549659122657 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,2.0,1.0,0.22,0.8])  |32.0  |-68.30971673360057 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,2.0,2.0,0.18,0.55]) |16.0  |-35.47202238977879 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,3.0,1.0,0.24,0.75]) |13.0  |-45.286139727894074|\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,4.0,1.0,0.24,0.75]) |1.0   |-37.66035987026809 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,4.0,2.0,0.16,0.59]) |5.0   |-33.666433500655266|\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,6.0,1.0,0.22,0.8])  |2.0   |-37.80659730309663 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,7.0,1.0,0.14,0.63]) |10.0  |-19.55443212556789 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,7.0,1.0,0.2,0.86])  |3.0   |-47.53044091550314 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,8.0,1.0,0.24,0.75]) |8.0   |-7.157240439764142 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,9.0,1.0,0.32,0.76]) |14.0  |21.071375249191444 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,13.0,1.0,0.08,0.35])|84.0  |63.93538741988339  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,15.0,1.0,0.12,0.28])|103.0 |104.1270624654403  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,17.0,1.0,0.12,0.28])|67.0  |119.37862218069228 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,17.0,1.0,0.24,0.6]) |91.0  |90.75217310815009  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,2.0,1.0,0.14,0.8])  |15.0  |-91.25655934349228 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,3.0,1.0,0.12,0.8])  |10.0  |-89.26944502418671 |\n",
      "+-----------------------------------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.predictions.show(n=20, truncate = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| 13.000498033759701|\n",
      "| 14.675212012651428|\n",
      "|  -32.1409108911277|\n",
      "|-1.6583186733249704|\n",
      "| 49.292213358281266|\n",
      "| 22.360021014691373|\n",
      "| -69.60317209286863|\n",
      "| 134.59329713283037|\n",
      "|  96.23523499374329|\n",
      "|  77.96455073678902|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    avg(residuals)|\n",
      "+------------------+\n",
      "|0.5084773049173261|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.groupBy().avg().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(abs(residuals))|\n",
      "+-------------------+\n",
      "|  105.2737009090852|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "df = test_results.residuals\n",
    "df.select(abs(df.residuals)).groupBy().avg().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtains means squarred error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.397747\n",
      "rootMeanSquaredError = 141.306\n",
      "meanAbsoluteError = 105.274\n"
     ]
    }
   ],
   "source": [
    "print(\"r2 = %g\"%test_results.r2)   # my model explains x % of the variance of the data\n",
    "print(\"rootMeanSquaredError = %g\"%test_results.rootMeanSquaredError)\n",
    "print(\"meanAbsoluteError = %g\"%test_results.meanAbsoluteError)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we dont have a good score.. We can do some :\n",
    "- decrease regularization parameter? \n",
    "- Add more features? Feature Engineering?\n",
    "- Polynomial Regression? other algorithms? Trees? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = lr_model.evaluate(data)\n",
    "pred = insights.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(season=1, yr=0, mnth=1, hr=0, holiday=0, workingday=0, weathersit=1, temp=0.24, hum=0.81, windspeed=0.0, dayOfWeek='Sat', days=0, demand=16, day_cat=0.0, features=SparseVector(11, {0: 1.0, 2: 1.0, 6: 1.0, 7: 0.24, 8: 0.81}), prediction=-79.87443723248417)]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(season=1, yr=0, mnth=1, hr=0, holiday=0, workingday=0, weathersit=1, temp=0.24, hum=0.81, windspeed=0.0, dayOfWeek='Sat', days=0, demand=16, day_cat=0.0, features=SparseVector(11, {0: 1.0, 2: 1.0, 6: 1.0, 7: 0.24, 8: 0.81}), prediction=-79.87443723248417, res_abs=95.87443723248417)]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_res = pred.withColumn('res_abs', abs(pred.prediction-pred.demand))\n",
    "pred_res.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get some insights per season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------+-----------------+-------------+\n",
      "|season|avg_abs_residual|avg_demand|stddev_prediction|stddev_demand|\n",
      "+------+----------------+----------+-----------------+-------------+\n",
      "|     1|           77.48|    111.11|            97.93|       119.22|\n",
      "|     2|          109.21|    208.34|           106.70|       188.36|\n",
      "|     3|          128.01|    236.02|           100.41|       197.71|\n",
      "|     4|          107.72|    198.87|            95.10|       182.97|\n",
      "+------+----------------+----------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.groupBy('season').agg(\n",
    "    format_number(avg('res_abs'), 2).alias('avg_abs_residual'),\n",
    "    format_number(avg('demand'), 2).alias('avg_demand'),\n",
    "    format_number(stddev('prediction'), 2).alias('stddev_prediction'),\n",
    "    format_number(stddev('demand'), 2).alias('stddev_demand')\n",
    ").sort('season').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get some insights per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------+-----------------+-------------+\n",
      "|mnth|avg_abs_residual|avg_demand|stddev_prediction|stddev_demand|\n",
      "+----+----------------+----------+-----------------+-------------+\n",
      "|   1|           68.52|     94.42|            93.21|        99.91|\n",
      "|   2|           79.71|    112.87|            99.46|       112.49|\n",
      "|   3|           95.32|    155.41|           104.39|       163.54|\n",
      "|   4|          106.06|    187.26|           109.72|       181.14|\n",
      "|   5|          108.43|    222.91|           102.66|       187.72|\n",
      "|   6|          124.61|    240.52|           103.73|       196.04|\n",
      "|   7|          131.05|    231.82|            97.13|       187.48|\n",
      "|   8|          126.71|    238.10|            94.32|       200.44|\n",
      "|   9|          127.09|    240.77|            96.91|       214.61|\n",
      "|  10|          116.95|    222.16|            96.45|       203.48|\n",
      "|  11|           99.23|    177.34|            91.96|       158.97|\n",
      "|  12|           85.17|    142.30|            89.31|       141.08|\n",
      "+----+----------------+----------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.groupBy('mnth').agg(\n",
    "    format_number(avg('res_abs'), 2).alias('avg_abs_residual'),\n",
    "    format_number(avg('demand'), 2).alias('avg_demand'),\n",
    "    format_number(stddev('prediction'), 2).alias('stddev_prediction'),\n",
    "    format_number(stddev('demand'), 2).alias('stddev_demand')\n",
    ").sort('mnth').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get some insights per day of week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get some insights per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------+-----------------+-------------+\n",
      "| hr|avg_abs_residual|avg_demand|stddev_prediction|stddev_demand|\n",
      "+---+----------------+----------+-----------------+-------------+\n",
      "|  0|           60.01|     53.90|            77.26|        42.31|\n",
      "|  1|           71.67|     33.38|            76.19|        33.54|\n",
      "|  2|           79.56|     22.87|            74.83|        26.58|\n",
      "|  3|           89.76|     11.73|            72.56|        13.24|\n",
      "|  4|           96.75|      6.35|            71.63|         4.14|\n",
      "|  5|           87.44|     19.89|            72.52|        13.20|\n",
      "|  6|           53.35|     76.04|            73.29|        55.08|\n",
      "|  7|          142.74|    212.06|            76.42|       161.44|\n",
      "|  8|          248.91|    359.01|            81.12|       235.19|\n",
      "|  9|           76.95|    219.31|            83.78|        93.70|\n",
      "| 10|           69.50|    173.67|            87.20|       102.21|\n",
      "| 11|           80.36|    208.14|            88.79|       127.50|\n",
      "| 12|           83.11|    253.32|            90.29|       145.08|\n",
      "| 13|           88.85|    253.66|            90.72|       148.11|\n",
      "| 14|           99.00|    240.95|            91.80|       147.27|\n",
      "| 15|           94.60|    251.23|            92.37|       144.63|\n",
      "| 16|           81.55|    311.98|            92.52|       148.68|\n",
      "| 17|          208.23|    461.45|            92.64|       232.66|\n",
      "| 18|          180.88|    425.51|            90.27|       224.64|\n",
      "| 19|           90.76|    311.52|            87.40|       161.05|\n",
      "+---+----------------+----------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.groupBy('hr').agg(\n",
    "    format_number(avg('res_abs'), 2).alias('avg_abs_residual'),\n",
    "    format_number(avg('demand'), 2).alias('avg_demand'),\n",
    "    format_number(stddev('prediction'), 2).alias('stddev_prediction'),\n",
    "    format_number(stddev('demand'), 2).alias('stddev_demand')\n",
    ").sort('hr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+----------+-----------------+-------------+\n",
      "|dayofweek|avg_abs_residual|avg_demand|stddev_prediction|stddev_demand|\n",
      "+---------+----------------+----------+-----------------+-------------+\n",
      "|      Fri|          100.54|    196.14|           114.42|       174.08|\n",
      "|      Mon|          106.41|    183.74|           110.65|       179.51|\n",
      "|      Sat|           99.24|    190.21|           114.15|       179.82|\n",
      "|      Sun|           99.16|    177.47|           108.74|       168.17|\n",
      "|      Thr|          111.47|    196.44|           111.45|       188.01|\n",
      "|      Tue|          112.73|    191.24|           109.87|       187.82|\n",
      "|      Wed|          112.46|    191.13|           114.64|       190.89|\n",
      "+---------+----------------+----------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.groupBy('dayofweek').agg(\n",
    "    format_number(avg('res_abs'), 2).alias('avg_abs_residual'),\n",
    "    format_number(avg('demand'), 2).alias('avg_demand'),\n",
    "    format_number(stddev('prediction'), 2).alias('stddev_prediction'),\n",
    "    format_number(stddev('demand'), 2).alias('stddev_demand')\n",
    ").sort('dayofweek').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get some insights per temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------+-----------------+-------------+\n",
      "|temp|avg_abs_residual|avg_demand|stddev_prediction|stddev_demand|\n",
      "+----+----------------+----------+-----------------+-------------+\n",
      "|0.02|           63.05|     41.88|            55.74|        80.88|\n",
      "|0.04|           65.20|     35.62|            54.68|        57.78|\n",
      "|0.06|           62.65|     42.00|            66.56|        30.58|\n",
      "|0.08|           51.96|     28.24|            68.73|        27.02|\n",
      "| 0.1|           67.78|     49.29|            71.93|        74.69|\n",
      "|0.12|           71.97|     58.42|            79.36|        70.43|\n",
      "|0.14|           70.05|     55.11|            85.32|        61.83|\n",
      "|0.16|           68.01|     65.58|            76.48|        73.08|\n",
      "|0.18|           63.89|     60.12|            73.14|        65.68|\n",
      "| 0.2|           69.59|     79.75|            75.25|       101.83|\n",
      "|0.22|           67.22|     69.91|            75.69|        93.25|\n",
      "|0.24|           77.15|     80.16|            77.47|       105.34|\n",
      "|0.26|           76.91|     87.96|            81.86|        98.66|\n",
      "|0.28|           83.43|    106.75|            79.84|       112.99|\n",
      "| 0.3|           84.29|    115.92|            82.74|       114.88|\n",
      "|0.32|           89.32|    134.23|            88.69|       134.11|\n",
      "|0.34|           82.29|    135.31|            88.85|       129.49|\n",
      "|0.36|           86.75|    147.84|            87.56|       128.82|\n",
      "|0.38|           85.16|    164.21|            89.20|       146.32|\n",
      "| 0.4|           92.43|    167.44|            85.20|       149.67|\n",
      "+----+----------------+----------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_res.groupBy('temp').agg(\n",
    "    format_number(avg('res_abs'), 2).alias('avg_abs_residual'),\n",
    "    format_number(avg('demand'), 2).alias('avg_demand'),\n",
    "    format_number(stddev('prediction'), 2).alias('stddev_prediction'),\n",
    "    format_number(stddev('demand'), 2).alias('stddev_demand')\n",
    ").sort('temp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model : add dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"bike_rental_dataset.csv\", inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|dayOfWeek|days|demand|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|      Sat|   0|    16|\n",
      "|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    40|\n",
      "|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    32|\n",
      "|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|    13|\n",
      "|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     1|\n",
      "|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|      Sat|   0|     1|\n",
      "|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|     2|\n",
      "|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|      Sat|   0|     3|\n",
      "|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     8|\n",
      "|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|      Sat|   0|    14|\n",
      "|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|      Sat|   0|    36|\n",
      "|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|      Sat|   0|    56|\n",
      "|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|      Sat|   0|    84|\n",
      "|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|      Sat|   0|    94|\n",
      "|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|      Sat|   0|   106|\n",
      "|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|      Sat|   0|   110|\n",
      "|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|      Sat|   0|    93|\n",
      "|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|      Sat|   0|    67|\n",
      "|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    35|\n",
      "|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    37|\n",
      "+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_indexer = StringIndexer(inputCol=\"season\", outputCol=\"season_index\")\n",
    "season_encoder = OneHotEncoder(inputCol=\"season_index\", outputCol=\"season_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_indexer = StringIndexer(inputCol=\"yr\", outputCol=\"year_index\")\n",
    "year_encoder = OneHotEncoder(inputCol=\"year_index\", outputCol=\"year_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_indexer = StringIndexer(inputCol=\"mnth\", outputCol=\"mnth_index\")\n",
    "month_encoder = OneHotEncoder(inputCol=\"mnth_index\", outputCol=\"mnth_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_indexer = StringIndexer(inputCol=\"hr\", outputCol=\"hr_index\")\n",
    "hour_encoder = OneHotEncoder(inputCol=\"hr_index\", outputCol=\"hr_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_indexer = StringIndexer(inputCol=\"dayOfWeek\", outputCol=\"weekday_index\")\n",
    "weekday_encoder = OneHotEncoder(inputCol=\"weekday_index\", outputCol=\"weekday_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_indexer = StringIndexer(inputCol=\"weathersit\", outputCol=\"weather_index\")\n",
    "weather_encoder = OneHotEncoder(inputCol=\"weather_index\", outputCol=\"weather_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_indexer = StringIndexer(inputCol=\"holiday\", outputCol=\"holiday_index\")\n",
    "holiday_encoder = OneHotEncoder(inputCol=\"holiday_index\", outputCol=\"holiday_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingday_indexer = StringIndexer(inputCol=\"workingday\", outputCol=\"workingday_index\")\n",
    "workingday_encoder = OneHotEncoder(inputCol=\"workingday_index\", outputCol=\"workingday_dummy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fit encoders and indexors to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = season_indexer.fit(data).transform(data)\n",
    "data = season_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = year_indexer.fit(data).transform(data)\n",
    "data = year_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = month_indexer.fit(data).transform(data)\n",
    "data = month_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hour_indexer.fit(data).transform(data)\n",
    "data = hour_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weekday_indexer.fit(data).transform(data)\n",
    "data = weekday_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weather_indexer.fit(data).transform(data)\n",
    "data = weather_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = holiday_indexer.fit(data).transform(data)\n",
    "data = holiday_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = workingday_indexer.fit(data).transform(data)\n",
    "data = workingday_encoder.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: int, yr: int, mnth: int, hr: int, holiday: int, workingday: int, weathersit: int, temp: double, hum: double, windspeed: double, dayOfWeek: string, days: int, demand: int, day_cat: double, features: vector, season_index: double, season_dummy: vector, year_index: double, year_dummy: vector, mnth_index: double, mnth_dummy: vector, hr_index: double, hr_dummy: vector, weekday_index: double, weekday_dummy: vector, weather_index: double, weather_dummy: vector, holiday_index: double, holiday_dummy: vector, workingday_index: double, workingday_dummy: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the dummy variables to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"season_dummy\",\n",
    "        \"year_dummy\",\n",
    "        \"mnth_dummy\",\n",
    "        \"hr_dummy\",\n",
    "        \"weekday_dummy\",\n",
    "        \"weather_dummy\",\n",
    "        \"holiday_dummy\",\n",
    "        \"workingday_dummy\",\n",
    "        \"temp\",\n",
    "        \"hum\",\n",
    "        \"windspeed\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assembler = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|demand|\n",
      "+--------------------+------+\n",
      "|(11,[0,2,6,7,8],[...|    16|\n",
      "|(11,[0,2,3,6,7,8]...|    40|\n",
      "|(11,[0,2,3,6,7,8]...|    32|\n",
      "|(11,[0,2,3,6,7,8]...|    13|\n",
      "|(11,[0,2,3,6,7,8]...|     1|\n",
      "|[1.0,0.0,1.0,5.0,...|     1|\n",
      "|(11,[0,2,3,6,7,8]...|     2|\n",
      "|(11,[0,2,3,6,7,8]...|     3|\n",
      "|(11,[0,2,3,6,7,8]...|     8|\n",
      "|(11,[0,2,3,6,7,8]...|    14|\n",
      "|[1.0,0.0,1.0,10.0...|    36|\n",
      "|[1.0,0.0,1.0,11.0...|    56|\n",
      "|[1.0,0.0,1.0,12.0...|    84|\n",
      "|[1.0,0.0,1.0,13.0...|    94|\n",
      "|[1.0,0.0,1.0,14.0...|   106|\n",
      "|[1.0,0.0,1.0,15.0...|   110|\n",
      "|[1.0,0.0,1.0,16.0...|    93|\n",
      "|[1.0,0.0,1.0,17.0...|    67|\n",
      "|[1.0,0.0,1.0,18.0...|    35|\n",
      "|[1.0,0.0,1.0,19.0...|    37|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_select = data_assembler.select(\"features\", \"demand\")\n",
    "data_select.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with dumy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_select.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|             17379|\n",
      "|   mean|189.46308763450142|\n",
      "| stddev| 181.3875990918646|\n",
      "|    min|                 1|\n",
      "|    max|               977|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_select.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|             12185|\n",
      "|   mean|188.91981945014362|\n",
      "| stddev|180.64641641321109|\n",
      "|    min|                 1|\n",
      "|    max|               970|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            demand|\n",
      "+-------+------------------+\n",
      "|  count|              5194|\n",
      "|   mean| 190.7375818251829|\n",
      "| stddev|183.12578416414303|\n",
      "|    min|                 1|\n",
      "|    max|               977|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dummy = LinearRegression(labelCol='demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, demand: int]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 12:33:05 WARN Instrumentation: [67faf139] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "lr_dummy_model = lr_dummy.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance = 12697.284990609\n",
      "mean absolute error = 105.44729536637277\n"
     ]
    }
   ],
   "source": [
    "summary_dummy = lr_dummy_model.summary\n",
    "print(\"explained variance =\", summary_dummy.explainedVariance)\n",
    "print(\"mean absolute error =\", summary_dummy.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3891238134036027"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dummy.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------+-------------------+\n",
      "|features                                       |demand|prediction         |\n",
      "+-----------------------------------------------+------+-------------------+\n",
      "|(11,[0,1,2,6,7,8],[2.0,1.0,3.0,1.0,0.58,0.68]) |156.0 |135.20393278863173 |\n",
      "|(11,[0,1,2,6,7,8],[3.0,1.0,6.0,1.0,0.64,0.83]) |116.0 |142.79254000302075 |\n",
      "|(11,[0,1,2,6,7,8],[4.0,1.0,12.0,1.0,0.26,0.81])|108.0 |59.86328899983247  |\n",
      "|(11,[0,1,2,6,7,8],[4.0,1.0,12.0,1.0,0.3,0.7])  |94.0  |92.36707091755346  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,1.0,1.0,0.22,0.8])  |40.0  |-80.47989898237392 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,2.0,2.0,0.18,0.55]) |16.0  |-41.37631251490641 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,3.0,2.0,0.16,0.59]) |8.0   |-46.921290267934474|\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,4.0,2.0,0.16,0.59]) |5.0   |-39.11266790697113 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,5.0,1.0,0.16,0.59]) |1.0   |-25.54381129337562 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,6.0,1.0,0.22,0.8])  |2.0   |-41.436787177557186|\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,7.0,1.0,0.14,0.63]) |10.0  |-23.28016668544037 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,8.0,1.0,0.24,0.75]) |8.0   |-10.533748445059638|\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,9.0,1.0,0.32,0.76]) |14.0  |17.84197813002013  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,15.0,1.0,0.12,0.28])|103.0 |101.19077405487039 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,17.0,1.0,0.24,0.6]) |91.0  |88.72676125230102  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,1.0,22.0,1.0,0.06,0.49])|30.0  |98.40058517042523  |\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,1.0,1.0,0.14,0.86]) |24.0  |-114.44536542274577|\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,3.0,1.0,0.12,0.8])  |10.0  |-92.85978184901681 |\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,6.0,1.0,0.22,0.6])  |7.0   |-2.6659141961695614|\n",
      "|(11,[0,2,3,6,7,8],[1.0,2.0,8.0,2.0,0.24,0.6])  |57.0  |12.81592080079894  |\n",
      "+-----------------------------------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_dummy.predictions.show(n=20, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lr_dummy_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         residuals|\n",
      "+------------------+\n",
      "| 14.21580771007154|\n",
      "| 40.90016103278441|\n",
      "|-61.66903515775698|\n",
      "|104.67127662141058|\n",
      "|  62.5768602498764|\n",
      "|42.768237888913056|\n",
      "| 53.84615272374398|\n",
      "| 23.20147699845979|\n",
      "| -49.8080187767971|\n",
      "| 110.0435796823062|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(residuals)|\n",
      "+-----------------+\n",
      "|1.342185169836071|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.groupBy().avg().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(abs(residuals))|\n",
      "+-------------------+\n",
      "| 107.02640182475514|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_results.residuals\n",
    "df.select(abs(df.residuals)).groupBy().avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.383123\n",
      "rootMeanSquaredError = 143.816\n",
      "meanAbsoluteError = 107.026\n"
     ]
    }
   ],
   "source": [
    "print(\"r2 = %g\"%test_results.r2)   # my model explains x % of the variance of the data\n",
    "print(\"rootMeanSquaredError = %g\"%test_results.rootMeanSquaredError)\n",
    "print(\"meanAbsoluteError = %g\"%test_results.meanAbsoluteError)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model by doing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train, test = data_select.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 142.116\n",
      "R2 score on test data: 0.3845824333360707\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search over\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "             .build())\n",
    "\n",
    "# Define the evaluation metric and the cross-validator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"demand\", metricName=\"rmse\")\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Evaluate the performance of the best model on the test data\n",
    "predictions = cvModel.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE on test data = {:.3f}\".format(rmse))\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(\"R2 score on test data: {}\".format(r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1100.8 KiB\n",
      "23/02/27 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1698.6 KiB\n",
      "23/02/27 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1334.1 KiB\n",
      "23/02/27 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/02/27 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1017.3 KiB\n",
      "23/02/27 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2037:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1645.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:36 WARN DAGScheduler: Broadcasting large task binary with size 1323.6 KiB\n",
      "23/02/27 13:05:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:05:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2082:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:38 WARN DAGScheduler: Broadcasting large task binary with size 1144.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:38 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2084:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:39 WARN DAGScheduler: Broadcasting large task binary with size 2016.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:40 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2086:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:41 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:44 WARN DAGScheduler: Broadcasting large task binary with size 1112.3 KiB\n",
      "23/02/27 13:05:44 WARN DAGScheduler: Broadcasting large task binary with size 1724.9 KiB\n",
      "23/02/27 13:05:47 WARN DAGScheduler: Broadcasting large task binary with size 1331.0 KiB\n",
      "23/02/27 13:05:47 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:05:48 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/02/27 13:05:48 WARN DAGScheduler: Broadcasting large task binary with size 1012.8 KiB\n",
      "23/02/27 13:05:49 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/02/27 13:05:49 WARN DAGScheduler: Broadcasting large task binary with size 1625.4 KiB\n",
      "23/02/27 13:05:54 WARN DAGScheduler: Broadcasting large task binary with size 1321.2 KiB\n",
      "23/02/27 13:05:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:05:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2229:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:56 WARN DAGScheduler: Broadcasting large task binary with size 1147.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:57 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2231:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:57 WARN DAGScheduler: Broadcasting large task binary with size 2019.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:05:59 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2233:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:00 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:03 WARN DAGScheduler: Broadcasting large task binary with size 1117.5 KiB\n",
      "23/02/27 13:06:03 WARN DAGScheduler: Broadcasting large task binary with size 1722.8 KiB\n",
      "23/02/27 13:06:06 WARN DAGScheduler: Broadcasting large task binary with size 1332.8 KiB\n",
      "23/02/27 13:06:06 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:06:07 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/02/27 13:06:07 WARN DAGScheduler: Broadcasting large task binary with size 1012.7 KiB\n",
      "23/02/27 13:06:08 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2331:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:08 WARN DAGScheduler: Broadcasting large task binary with size 1642.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:13 WARN DAGScheduler: Broadcasting large task binary with size 1323.2 KiB\n",
      "23/02/27 13:06:13 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:06:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2376:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:15 WARN DAGScheduler: Broadcasting large task binary with size 1150.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:15 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2378:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:16 WARN DAGScheduler: Broadcasting large task binary with size 2037.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:17 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2380:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:18 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:21 WARN DAGScheduler: Broadcasting large task binary with size 1104.5 KiB\n",
      "23/02/27 13:06:21 WARN DAGScheduler: Broadcasting large task binary with size 1696.5 KiB\n",
      "23/02/27 13:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1330.8 KiB\n",
      "23/02/27 13:06:25 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:06:25 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/02/27 13:06:26 WARN DAGScheduler: Broadcasting large task binary with size 1007.6 KiB\n",
      "23/02/27 13:06:26 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/02/27 13:06:27 WARN DAGScheduler: Broadcasting large task binary with size 1618.8 KiB\n",
      "23/02/27 13:06:31 WARN DAGScheduler: Broadcasting large task binary with size 1324.1 KiB\n",
      "23/02/27 13:06:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:06:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2523:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:33 WARN DAGScheduler: Broadcasting large task binary with size 1147.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:34 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2525:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:35 WARN DAGScheduler: Broadcasting large task binary with size 2036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:36 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2527:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:37 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1111.0 KiB\n",
      "23/02/27 13:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1719.4 KiB\n",
      "23/02/27 13:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1332.0 KiB\n",
      "23/02/27 13:06:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/02/27 13:06:44 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/02/27 13:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1016.3 KiB\n",
      "23/02/27 13:06:45 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2625:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1649.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:50 WARN DAGScheduler: Broadcasting large task binary with size 1322.7 KiB\n",
      "23/02/27 13:06:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/02/27 13:06:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2670:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:52 WARN DAGScheduler: Broadcasting large task binary with size 1146.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:53 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2672:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:53 WARN DAGScheduler: Broadcasting large task binary with size 2032.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:55 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2674:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:56 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1230.2 KiB\n",
      "23/02/27 13:07:00 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2695:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1167.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:02 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2697:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:03 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:04 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2699:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 13:07:05 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 143.12, R2 Score: 0.38\n",
      "Decision Tree Regression - RMSE: 57.67, R2 Score: 0.90\n",
      "Random Forest Regression - RMSE: 70.99, R2 Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and preprocess your data\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"demand\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"demand\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"demand\")\n",
    "\n",
    "# Define the pipelines for each algorithm\n",
    "lr_pipeline = Pipeline(stages=[lr])\n",
    "dt_pipeline = Pipeline(stages=[dt])\n",
    "rf_pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the parameter grids to search over for each algorithm\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "                .build())\n",
    "dt_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(dt.maxDepth, [2, 5, 10])\n",
    "                .build())\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(rf.numTrees, [10, 50, 100])\n",
    "                .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "                .build())\n",
    "\n",
    "# Define the evaluation metric and the cross-validators for each algorithm\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"demand\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"demand\", metricName=\"r2\")\n",
    "lr_cv = CrossValidator(estimator=lr_pipeline, estimatorParamMaps=lr_paramGrid, evaluator=evaluator, numFolds=5)\n",
    "dt_cv = CrossValidator(estimator=dt_pipeline, estimatorParamMaps=dt_paramGrid, evaluator=evaluator, numFolds=5)\n",
    "rf_cv = CrossValidator(estimator=rf_pipeline, estimatorParamMaps=rf_paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the cross-validators to the training data\n",
    "lr_cvModel = lr_cv.fit(train)\n",
    "dt_cvModel = dt_cv.fit(train)\n",
    "rf_cvModel = rf_cv.fit(train)\n",
    "\n",
    "# Evaluate the performance of each algorithm on the test data\n",
    "lr_predictions = lr_cvModel.transform(test)\n",
    "lr_rmse = evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(\"Linear Regression - RMSE: {:.2f}, R2 Score: {:.2f}\".format(lr_rmse, lr_r2))\n",
    "                                                                  \n",
    "dt_predictions = dt_cvModel.transform(test)\n",
    "dt_rmse = evaluator.evaluate(dt_predictions)\n",
    "dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "print(\"Decision Tree Regression - RMSE: {:.2f}, R2 Score: {:.2f}\".format(dt_rmse, dt_r2))\n",
    "\n",
    "rf_predictions = rf_cvModel.transform(test)\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "print(\"Random Forest Regression - RMSE: {:.2f}, R2 Score: {:.2f}\".format(rf_rmse, rf_r2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
